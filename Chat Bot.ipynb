{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Practical Guide to Building a Conversational Chat Bot\n",
    "\n",
    "#### **By:** <b>Imonmion Emmanuel</b>\n",
    "\n",
    "#### For this Notebook we would using some Python Packages\n",
    "\n",
    "__Packages we would use For This Notebook:__\n",
    "1. Json\n",
    "2. Numpy\n",
    "3. NLTK\n",
    "4. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Conversational Chat Bot\n",
    "\n",
    "In the Essence of the world, it is a robot, that enables a machine to simulate human like conversations.\n",
    "To achieve this, the user interface needs to be as humanlike and conversational as possible. A conversational chatbot must understand the users intents and how to respond to the user.\n",
    "\n",
    "Fundamentally a chatbot turns raw data into conversation, The two keys bits of data that a chatbot needs to process are what people are saying to it and what it needs to respond. The easiet example to understand is a simple customer service chat Bot.\n",
    "\n",
    "__If you intend to build a chatbot for a Customer or a Company you would need to understand the customer/company workflow, customer service and needs and Data, Beacuse Data is the key to develop a truly conversational chatbot__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Libraries to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "import nltk \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening our Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chat.json\") as file:\n",
    "    bot = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through the opened Json file and appending it to different list for further iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "intent_part_x = []\n",
    "intent_part_y = []\n",
    "\n",
    "\n",
    "for intent in bot[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        token = nltk.wordpunct_tokenize(pattern) #Tokenizing the pattern\n",
    "        #Since token is a list no need of appending we just use the function extend\n",
    "        words.extend(token)\n",
    "        intent_part_x.append(token)\n",
    "        intent_part_y.append(intent[\"tag\"])\n",
    "\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the Word using NLTK Stemmer and Sorting it Alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [ps.stem(w.lower()) for w in words]\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating The Train and Output List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of word list from the labels list\n",
    "\n",
    "out_empty = [0 for i,col in enumerate(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intent_part_x is the list that contains all the available Input for the user in the Json File\n",
    "\n",
    "#Iterating through the file to create a list of bag of words to check if a user Input was present in the Json File\n",
    "\n",
    "#If present then append 1\n",
    "\n",
    "for x, doc in enumerate(intent_part_x):\n",
    "    bow = []\n",
    "\n",
    "    token = [ps.stem(w) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in token:\n",
    "            bow.append(1)\n",
    "        else:\n",
    "            bow.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(intent_part_y[x])] = 1\n",
    "\n",
    "    train.append(bow)  #Creating the Train \n",
    "    output.append(output_row) #Creating the output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concverting it to an numpy array since deep learning deals with array\n",
    "\n",
    "train = np.array(train)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Array\")\n",
    "\n",
    "print(train[5])\n",
    "\n",
    "\n",
    "print(f\"\\nLength of train array: {len(train[5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Deep Learning Neural Net\n",
    "\n",
    "### We would using only Three Layer since it is not a complex File \n",
    "\n",
    "#### We would be using Relu activation function and The Softmax activation since we are expected to have multiple outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(output[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We would be using the Stochastic Gradient Descent and the Nesterov Accelerated Gradient Descent\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Model \n",
    "model.fit(train, output, epochs=200, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Functions that would enable the user interact with the chat bot runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s,words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    This functions create a bag of word for the sentence the user enters\n",
    "    \n",
    "    Output:\n",
    "    It returns a numpy array to be used \n",
    "    Note: The Numpy array was reshaped to an array of 79 because the length of our train[0] is 79\n",
    "    If you make changes or edit the json file to suit your needs then the Length of your train will change\n",
    "    Do well to play around the codes, make changes to suit your need and understand how it works\n",
    "    \"\"\"\n",
    "    bag = [ 0 for i in range(len(words))]\n",
    "\n",
    "    s_words_token = nltk.wordpunct_tokenize(s)\n",
    "    s_words_token = [ps.stem(word.lower()) for word in s_words_token]\n",
    "\n",
    "    for se in s_words_token:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "    \n",
    "    return np.array(bag).reshape(-1,79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    This is function that Activate the ChatBot for the User to Interact with it\n",
    "    \n",
    "    Output:\n",
    "    It gives a Responses randomly.\n",
    "    We selected a threshold to prevent it from giving out irrelevatnt response to the user\n",
    "    You can quit talking with the Chat Bot by type quit in the chat Column\n",
    "    \n",
    "    Note: The Threshold was not selected Randomly, it was done after sereies of conversation with the bot to know,\n",
    "    which threshold it starts giving out irrelevant responses if the responses is not in it database\n",
    "    \"\"\"\n",
    "    print(\"Start Talking with the Bot,To Stop type quit\")\n",
    "    while True:\n",
    "        inp = input(\"You :   \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break \n",
    "\n",
    "        input_data = [bag_of_words(inp, words)]\n",
    "        results = model.predict(input_data)[0]\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        \n",
    "        #To see how the model perfoms on data that are not in the database\n",
    "        #print(f\"Model Prediction: {results[results_index]}\")\n",
    "        \n",
    "        if results[results_index] >= 0.794:\n",
    "            #Looping through the json file\n",
    "            for tags in bot[\"intents\"]:\n",
    "                if tags[\"tag\"] == tag:\n",
    "                    responses = tags[\"responses\"]\n",
    "            print(np.random.choice(responses))\n",
    "        \n",
    "        else:\n",
    "            print(\"I dont quite Understand, Ask another question\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Interaction with the ChatBot Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion…\n",
    "\n",
    "Building a ChatBot is not Rocket Science you can Build a chatbot for yourself and integrate it to your website, applications,or other social media platform to help keeps your conversation on the go even if you are not online.\n",
    "\n",
    "You can fork out this repo and work on it to suit your own need things you need to do is edit the Json File to how best you want your chatbot to interact with the user.\n",
    "\n",
    "As your Responses, Tag, Intents increase you would also have to increase the the layers in your model to have a better accuracy and responses for the chatbot.\n",
    "\n",
    "You can read about about the __Nesterov Accelerated Gradient Descent__\n",
    "\n",
    "__Natural Language Processing__ is an exciting Field that is fast growing you can also try other Deep Learning Framework like Fast.ai, PyTorch or Theano to see how your data learn and how the chatbot works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect with me on [twitter](https://www.twitter.com/imonemmanuel).\n",
    "\n",
    "#### Connect with me on [linkedin](https://www.linkedin.com/in/imonemmanuel)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
